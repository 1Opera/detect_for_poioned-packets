{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals, annotations\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# ----------------- Spark 相关导入 -----------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, rand, monotonically_increasing_id, input_file_name, regexp_extract\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, DoubleType, LongType, BooleanType\n",
    "\n",
    "# ----------------- 自定义工具导入 -----------------\n",
    "# 请确保 utils、ml.utils 模块在 Python 环境中可用\n",
    "from utils import load_config\n",
    "from ml.utils import load_application_classification_cnn_model, load_traffic_classification_cnn_model, normalise_cm\n",
    "\n",
    "# ----------------- PyTorch 相关导入 -----------------\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ----------------- ART、聚类、PCA 和可视化相关导入 -----------------\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.defences.detector.poison import ClusteringAnalyzer\n",
    "from art.defences.detector.poison.poison_filtering_defence import PoisonFilteringDefence\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# # 配置设备\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 直接指定使用 CPU 设备\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num=15\n",
    "labels = [str(i) for i in range(0, class_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from ml.dataset import dataset_collate_function\n",
    "import datasets\n",
    "import multiprocessing\n",
    "def train_dataloader(parquet_path):\n",
    "    # expect to get train folder\n",
    "    #print(\"self.hparams.data_path\",self.hparams.data_path)\n",
    "    # data_path=self.hparams.data_path\n",
    "    # df = pd.read_parquet(data_path)\n",
    "    # print(df.head())\n",
    "    #dataset_dict = datasets.load_dataset(self.hparams.data_path)\n",
    "    dataset_dict = datasets.load_dataset(parquet_path, keep_in_memory=False)\n",
    "    dataset = dataset_dict[list(dataset_dict.keys())[0]]\n",
    "    try:\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "    except:\n",
    "        num_workers = 1\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        #batch_size=4,\n",
    "        batch_size=128,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=dataset_collate_function,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    print(\"num_workers:\",num_workers)\n",
    "    return dataloader\n",
    "\n",
    "parquet_path = \"train.parquet\"\n",
    "dataloader=train_dataloader(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"application_classification.cnn.model\"\n",
    "from ml.utils import load_application_classification_cnn_model\n",
    "model = load_application_classification_cnn_model(model_path, gpu=True)\n",
    "model.eval()  # 设为评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器（此处仅用于 ART 分类器的构建）\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 15\n",
    "input_dimension = 1500  # 根据模型实际输入尺寸设置\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(input_dimension,),  # 例如信号长度为1500\n",
    "    nb_classes=class_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改数据收集部分\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_is_poisoned = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    # 获取字典中各字段，并转移到模型所在设备（如 GPU）\n",
    "    x_batch = batch[\"feature\"].float().to(model.device)\n",
    "    y_batch = batch[\"label\"].long().to(model.device)\n",
    "    poisoned_batch = batch[\"is_poisoned\"].bool().to(model.device)\n",
    "    \n",
    "    # 立即转移到CPU并释放GPU内存\n",
    "    all_features.append(x_batch.cpu().numpy())\n",
    "    all_labels.append(y_batch.cpu().numpy())\n",
    "    all_is_poisoned.append(poisoned_batch.cpu().numpy())\n",
    "    \n",
    "    # 清理GPU内存\n",
    "    del x_batch, y_batch, poisoned_batch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 最后再合并\n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "all_is_poisoned = np.concatenate(all_is_poisoned, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.detector.poison import ActivationDefence\n",
    "# 设置批次大小\n",
    "BATCH_SIZE = 1000  # 可以根据GPU内存调整\n",
    "\n",
    "# 初始化结果列表\n",
    "all_activations = []\n",
    "all_labels_batch = []\n",
    "all_is_poisoned_batch = []\n",
    "\n",
    "# 分批处理数据\n",
    "for i in range(0, len(all_features), BATCH_SIZE):\n",
    "    # 获取当前批次\n",
    "    batch_features = all_features[i:i+BATCH_SIZE]\n",
    "    batch_labels = all_labels[i:i+BATCH_SIZE]\n",
    "    batch_is_poisoned = all_is_poisoned[i:i+BATCH_SIZE]\n",
    "    \n",
    "    # 创建当前批次的defence\n",
    "    defence_batch = ActivationDefence(\n",
    "        classifier=classifier,\n",
    "        x_train=batch_features,\n",
    "        y_train=batch_labels,\n",
    "        generator=None,\n",
    "        ex_re_threshold=None\n",
    "    )\n",
    "    \n",
    "    # 获取当前批次的激活\n",
    "    try:\n",
    "        # 获取激活\n",
    "        activations = defence_batch._get_activations()\n",
    "        \n",
    "        # 将激活转移到CPU并存储\n",
    "        if isinstance(activations, np.ndarray):\n",
    "            all_activations.append(activations)\n",
    "        else:\n",
    "            all_activations.append(activations.cpu().numpy())\n",
    "            \n",
    "        # 存储标签和投毒信息\n",
    "        all_labels_batch.append(batch_labels)\n",
    "        all_is_poisoned_batch.append(batch_is_poisoned)\n",
    "        \n",
    "        # 清理GPU内存\n",
    "        del defence_batch\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"处理批次 {i//BATCH_SIZE + 1}/{len(all_features)//BATCH_SIZE + 1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理批次 {i//BATCH_SIZE + 1} 时出错: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# 合并所有批次的结果\n",
    "all_activations = np.concatenate(all_activations, axis=0)\n",
    "all_labels_batch = np.concatenate(all_labels_batch, axis=0)\n",
    "all_is_poisoned_batch = np.concatenate(all_is_poisoned_batch, axis=0)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 创建防御对象并设置FastICA\n",
    "final_defence = ActivationDefence(\n",
    "    classifier=classifier,\n",
    "    x_train=all_features,\n",
    "    y_train=all_labels,\n",
    "    generator=None,\n",
    "    ex_re_threshold=None,\n",
    ")\n",
    "# 设置降维方法为FastICA\n",
    "final_defence.reduce = \"PCA\"\n",
    "final_defence.clustering_method = \"KMeans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 进行防御检测\n",
    "final_defence.activations_by_class = final_defence._segment_by_class(all_activations, all_labels)\n",
    "(\n",
    "    final_defence.clusters_by_class,\n",
    "    final_defence.red_activations_by_class,\n",
    ") = final_defence.cluster_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 基于激活聚类的中毒检测 ----------------\n",
    "\n",
    "# 运行检测（传入特征和对应标签）\n",
    "report, is_clean_pred_lst = final_defence.detect_poison()\n",
    "# 转成布尔：True=clean, False=poison\n",
    "is_clean_pred = np.array(is_clean_pred_lst, dtype=bool)\n",
    "\n",
    "# all_is_poisoned_batch: True=poisoned, False=clean\n",
    "# ground‐truth is_clean: True=clean, False=poison\n",
    "is_clean_gt = ~all_is_poisoned_batch\n",
    "\n",
    "result_json = final_defence.evaluate_defence(is_clean=is_clean_gt)\n",
    "import json\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印结果\n",
    "print(f\"特征维度: {all_features.shape}\")\n",
    "print(f\"标签数量: {len(all_labels)}\")\n",
    "print(f\"投毒样本比例: {np.mean(all_is_poisoned):.2%}\")\n",
    "print(\"检测评估结果：\", result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 如果 result_json 是字符串，先转换为字典\n",
    "if isinstance(result_json, str):\n",
    "    result_json = json.loads(result_json)\n",
    "\n",
    "class_2_result = result_json[\"class_2\"]\n",
    "\n",
    "TP_num = class_2_result[\"TruePositive\"][\"numerator\"]\n",
    "FN_num = class_2_result[\"FalseNegative\"][\"numerator\"]\n",
    "FP_num = class_2_result[\"FalsePositive\"][\"numerator\"]\n",
    "TN_num = class_2_result[\"TrueNegative\"][\"numerator\"]\n",
    "\n",
    "total_positive = TP_num + FN_num\n",
    "total_negative = FP_num + TN_num\n",
    "\n",
    "if total_positive != 0:\n",
    "    TPR = TP_num / total_positive * 100  # 检出率 (TPR)\n",
    "    FNR = FN_num / total_positive * 100         # 漏检率 (FNR)\n",
    "else:\n",
    "    TPR = None\n",
    "    FNR = None\n",
    "\n",
    "if total_negative != 0:\n",
    "    FPR = FP_num / total_negative * 100   # 误报率 (FPR)\n",
    "else:\n",
    "    FPR = None\n",
    "\n",
    "print(\"\\n检测性能指标：\")\n",
    "if TPR is not None:\n",
    "    print(f\"检出率: {TPR:.2f}%\")\n",
    "else:\n",
    "    print(\"检出率: 未定义（无正样本）\")\n",
    "\n",
    "if FPR is not None:\n",
    "    print(f\"误报率：{FPR:.2f}%\")\n",
    "else:\n",
    "    print(\"误报率: 未定义（无正样本）\")\n",
    "\n",
    "if FNR is not None:\n",
    "    print(f\"漏检率: {FNR:.2f}%\")\n",
    "else:\n",
    "    print(\"漏检率: 未定义（无正样本）\")\n",
    "\n",
    "print(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化计数器和累加器\n",
    "valid_tp_count = 0  # 有效TruePositive类别数\n",
    "valid_fn_count = 0  # 有效FalseNegative类别数\n",
    "total_fp_count = 0  # 所有类别数（用于FalsePositive）\n",
    "\n",
    "tp_sum = 0  # TruePositive累加\n",
    "fn_sum = 0  # FalseNegative累加\n",
    "fp_sum = 0  # FalsePositive累加\n",
    "\n",
    "# 遍历所有类别\n",
    "for class_name, metrics in result_json.items():\n",
    "    # TruePositive\n",
    "    if metrics[\"TruePositive\"][\"rate\"] != -1:\n",
    "        tp_sum += metrics[\"TruePositive\"][\"rate\"]\n",
    "        valid_tp_count += 1\n",
    "    \n",
    "    # FalseNegative\n",
    "    if metrics[\"FalseNegative\"][\"rate\"] != -1:\n",
    "        fn_sum += metrics[\"FalseNegative\"][\"rate\"]\n",
    "        valid_fn_count += 1\n",
    "    \n",
    "    # FalsePositive（所有类别都有效）\n",
    "    fp_sum += metrics[\"FalsePositive\"][\"rate\"]\n",
    "    total_fp_count += 1\n",
    "\n",
    "# 计算平均指标\n",
    "avg_detection_rate = tp_sum / valid_tp_count if valid_tp_count > 0 else 0\n",
    "avg_miss_rate = fn_sum / valid_fn_count if valid_fn_count > 0 else 0\n",
    "avg_false_alarm_rate = fp_sum / total_fp_count\n",
    "\n",
    "print(\"\\n所有类别的平均检测性能指标：\")\n",
    "print(f\"平均检出率: {avg_detection_rate:.2f}%\")\n",
    "print(f\"平均误报率: {avg_false_alarm_rate:.2f}%\")\n",
    "print(f\"平均漏检率: {avg_miss_rate:.2f}%\")\n",
    "\n",
    "print(\"\\n各类别详细指标：\")\n",
    "for class_name, metrics in result_json.items():\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    tp_rate = metrics[\"TruePositive\"][\"rate\"]\n",
    "    fn_rate = metrics[\"FalseNegative\"][\"rate\"]\n",
    "    fp_rate = metrics[\"FalsePositive\"][\"rate\"]\n",
    "    \n",
    "    print(f\"检出率: {tp_rate if tp_rate != -1 else 'N/A'}%\")\n",
    "    print(f\"误报率: {fp_rate:.2f}%\")\n",
    "    print(f\"漏检率: {fn_rate if fn_rate != -1 else 'N/A'}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. 可视化：每个类别随机选取 50 个正常样本并绘制实心方块，投毒样本绘制红色空心三角形\n",
    "# import numpy as np\n",
    "\n",
    "# # 如果 red_acts_dict 是列表\n",
    "# red_acts_list = final_defence.red_activations_by_class  # 其实是一个 list\n",
    "\n",
    "# all_red_acts = []\n",
    "# all_labels_arr = []\n",
    "\n",
    "# # 如果你知道它是 [class_0_acts, class_1_acts, ...] 这种结构，\n",
    "# # 或文档说明第 i 个元素对应第 i 个类别，可以使用枚举\n",
    "# for i, acts in enumerate(red_acts_list):\n",
    "#     # acts 就是第 i 个类别的降维激活\n",
    "#     all_red_acts.append(acts)\n",
    "#     # 如果你能对应上类别标签，假设 i 就是类别标签，或者另有某个映射\n",
    "#     # 这里演示假设 i 即为类别\n",
    "#     all_labels_arr.extend([i]*len(acts))\n",
    "\n",
    "# all_red_acts = np.concatenate(all_red_acts, axis=0)\n",
    "# all_labels_arr = np.array(all_labels_arr)\n",
    "# print(all_labels_arr)\n",
    "\n",
    "\n",
    "# print(\"合并后的降维激活：\", all_red_acts.shape)\n",
    "# print(\"合并后的标签：\", all_labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_2d=all_red_acts\n",
    "# all_labels_batch=all_labels_arr\n",
    "\n",
    "# plt.figure(figsize=(10, 8),dpi=1200)\n",
    "# unique_labels = np.unique(all_labels_batch)\n",
    "\n",
    "# # 给不同类别分配颜色\n",
    "# colors = plt.cm.get_cmap(\"tab20b\", len(unique_labels))\n",
    "# # colors = plt.cm.get_cmap(\"plasma\", len(unique_labels))\n",
    "# label_to_color = {lab: colors(i) for i, lab in enumerate(unique_labels)}\n",
    "\n",
    "# n_per_class = 50\n",
    "\n",
    "# for lab in unique_labels:\n",
    "#     # 找到该类别、且未投毒的样本索引\n",
    "#     idx_clean_mask = np.where((all_labels_batch == lab) & (~all_is_poisoned_batch))[0]\n",
    "#     if len(idx_clean_mask) == 0:\n",
    "#         continue\n",
    "#     # 随机选取 50 个，或不足 50 个则全部选\n",
    "#     select_num = min(n_per_class, len(idx_clean_mask))\n",
    "#     selected_idx = np.random.choice(idx_clean_mask, size=select_num, replace=False)\n",
    "#     if lab != 2:\n",
    "#         # 绘制散点（实心方块）\n",
    "#         plt.scatter(\n",
    "#             features_2d[selected_idx, 0],\n",
    "#             features_2d[selected_idx, 1],\n",
    "#             color=label_to_color[lab],\n",
    "#             marker='s',  # 实心方块\n",
    "#             s=100,\n",
    "#             alpha=0.8,\n",
    "#             # label=f\"Other Class (Clean)\" if lab not in plt.gca().get_legend_handles_labels()[1] else \"\"\n",
    "#             # 为防止重复图例，每类只在首次绘制时加 legend\n",
    "#         )\n",
    "#     if lab == 2:\n",
    "#         # 绘制散点（）\n",
    "#         plt.scatter(\n",
    "#             features_2d[selected_idx, 0],\n",
    "#             features_2d[selected_idx, 1],\n",
    "#             color=\"#C82423\",\n",
    "#             marker='.',  # \n",
    "#             s=130,\n",
    "#             alpha=0.8,\n",
    "#             label=f\"Target Class (Clean)\" if lab not in plt.gca().get_legend_handles_labels()[1] else \"\"\n",
    "#             # 为防止重复图例，每类只在首次绘制时加 legend\n",
    "#         )\n",
    "# poison_idx = np.where(all_is_poisoned_batch)[0]\n",
    "\n",
    "# # 如果投毒样本超过50个，则随机选取50个\n",
    "# if len(poison_idx) > 50:\n",
    "#     np.random.shuffle(poison_idx)\n",
    "#     poison_idx = poison_idx[:50]\n",
    "\n",
    "# if len(poison_idx) > 0:\n",
    "#     plt.scatter(\n",
    "#         features_2d[poison_idx, 0],\n",
    "#         features_2d[poison_idx, 1],\n",
    "#         marker='^',       # 三角形\n",
    "#         facecolors='none', \n",
    "#         edgecolors='red',\n",
    "#         s=140,\n",
    "#         alpha=0.8,\n",
    "#         linewidth=1.5,\n",
    "#         label='Poisoned'\n",
    "#     )\n",
    "\n",
    "# plt.title(\"Feature Representation\")\n",
    "# # plt.xlabel(\"Principal Component 1\")\n",
    "# # plt.ylabel(\"Principal Component 2\")\n",
    "# # plt.xlabel(\"Principal Component 1\")\n",
    "# # plt.ylabel(\"Principal Component 2\")\n",
    "# plt.grid(True, linestyle='--', alpha=0.3)\n",
    "# plt.legend(loc='best')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"pca_subsampled_0.1-2.png\",dpi=1200)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-packet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
